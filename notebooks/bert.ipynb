{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3.5.1 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (3.5.1)\r\n",
      "Requirement already satisfied: protobuf in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from transformers==3.5.1) (3.15.1)\r\n",
      "Requirement already satisfied: filelock in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from transformers==3.5.1) (3.0.12)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from transformers==3.5.1) (4.57.0)\r\n",
      "Requirement already satisfied: sacremoses in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from transformers==3.5.1) (0.0.43)\r\n",
      "Requirement already satisfied: packaging in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from transformers==3.5.1) (20.9)\r\n",
      "Requirement already satisfied: requests in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from transformers==3.5.1) (2.25.1)\r\n",
      "Requirement already satisfied: tokenizers==0.9.3 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from transformers==3.5.1) (0.9.3)\r\n",
      "Requirement already satisfied: numpy in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from transformers==3.5.1) (1.20.1)\r\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from transformers==3.5.1) (0.1.91)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from transformers==3.5.1) (2020.11.13)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from packaging->transformers==3.5.1) (2.4.7)\r\n",
      "Requirement already satisfied: six>=1.9 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from protobuf->transformers==3.5.1) (1.15.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from requests->transformers==3.5.1) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from requests->transformers==3.5.1) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from requests->transformers==3.5.1) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from requests->transformers==3.5.1) (1.26.3)\r\n",
      "Requirement already satisfied: joblib in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from sacremoses->transformers==3.5.1) (1.0.1)\r\n",
      "Requirement already satisfied: click in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from sacremoses->transformers==3.5.1) (7.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per prima cosa importo da pachetto hugginface Bert il suo tokenizer WordPiece e pytorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Successicvamente dobbiamo scaricare il modello Bert, scegliamo il modello base con 12 encoder, 12 teste di attenzione\n",
    "per layer e 764 unità di attivazione per ogni rete feedward per gli encoder.\n",
    "chiamiamo il metodo from_pretrained di BertModel e selezionare il modello base passiamoli la stringa \"bert-base-uncased\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "lo stesso facciamo per il tokenizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora, vediamo come preelaborare l'input prima di inviarlo a BERT."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "sentence = \"I love Paris\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'paris']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora aggiungiamo i token speciali, quello  di classificazione [CLS] e quello di fine frase [SEP]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'paris', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['[CLS]']+tokens+['[SEP]']\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La lista dei tokens è 5, ma supponiamo che dobbiamo avere un numero di tokens fisso diciamo che la lista\n",
    "deve essere lunga 7. Per raggiungere questo obbiettivo possiamo pensare di riempirla con un token speciale\n",
    "che come unico scopo sia solo quello di riempire la lista dei token, [PAD]."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tokens = tokens+ ['[PAD]']+ ['[PAD]']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'paris', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A questo punto dobbiamo dire al nostro modello quali sono i token reali e quali no ([PAD]),\n",
    "per fare questo dobbiamo definire un oggetto chiamato attention mask (da non confondere per la matrice di attenzione)\n",
    "Attention mask assegna 1 ai token diversi da [PAD] e 0 a [PAD]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "attention_mask = [1 if token != '[PAD]' else 0 for token in tokens]\n",
    "print(attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora facciamo un'altro passo che nella teoria non viene detto.\n",
    "I tokens non possono rimanere parole, ma devono essere convertite in numeri perchè gli algoritmi lavorano\n",
    "su questo."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2293, 3000, 102, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dall'output, possiamo osservare che ogni token è mappato a un ID token univoco.\n",
    "Il prossimo passo è convertite la maschera di attenzione e la lista tokens_id in tensori con pytorch\n",
    "si consiglia di portarli alla gpu\n",
    "Si raccomanda di fare unsqueeze(0) per aumentare le dimensioni del tensore, in quanto la semplice lista\n",
    "non permette di rappresentare un vettore riga che è caratterizzato da 2 dimensioni (riga, colonna),\n",
    "senza unsqueeze(0) non avremmo la rappresentazione algebrica in quanto sarebbe su un unica dimensione e rappresentebbe\n",
    "solo il numero di elementi che compongono il tensore che in questo caso sarebbe solo 7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(tokens_id).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora dobbiamo ottenere gli embedding.\n",
    "Il nostro Bert restitusice due oggetti, analizziamoli:\n",
    "hidden_rep consiste nella rappresentazione finale del bert le famose R_CLS, R_I, R_love, R_Paris, R_SEP, R_PAD.\n",
    "Infatti la grandezza del tensore è [1,7, 768], 1 perchè abbiamo una frase, 7, perchè abbiamo 7 token e 768\n",
    "perchè l'ultimo encoder di BERT (ma come tutti gli encoder al suo interno) ha una rete feedforward con 768 unità\n",
    "che altro non rappresentano le dimensioni di embedding di ogni singolo token.\n",
    "Infine abbiamo cls_head che altro non è la rappresentazione finale del token speciale di classificazione R_CLS ,\n",
    "che conterrà l'intera informazione aggregata della frase I love Paris, la sua dimensione è [1,768]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "hidden_rep, cls_head = model(token_ids, attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_rep: \n",
      " tensor([[[-0.0719,  0.2163,  0.0047,  ..., -0.5865,  0.2262,  0.1981],\n",
      "         [ 0.2236,  0.6536, -0.2294,  ..., -0.3547,  0.5517, -0.2367],\n",
      "         [ 1.0410,  0.7755,  1.0335,  ..., -0.5621,  0.5218, -0.0852],\n",
      "         ...,\n",
      "         [ 0.6156,  0.1036, -0.1875,  ..., -0.3799, -0.7008, -0.3500],\n",
      "         [ 0.0791,  0.4287,  0.4147,  ..., -0.2417,  0.2403,  0.0378],\n",
      "         [-0.0165,  0.2459,  0.4566,  ..., -0.2179,  0.1876,  0.0228]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "cls_head: \n",
      " tensor([[-9.0660e-01, -3.4189e-01, -3.3729e-01,  7.7140e-01,  6.0975e-02,\n",
      "         -1.0525e-01,  9.0143e-01,  2.5822e-01, -2.7881e-01, -9.9997e-01,\n",
      "         -1.0322e-01,  7.4772e-01,  9.8521e-01,  5.9799e-02,  9.4447e-01,\n",
      "         -5.9859e-01, -2.0539e-01, -5.7386e-01,  3.7684e-01, -7.5183e-01,\n",
      "          6.6604e-01,  9.9584e-01,  4.2231e-01,  2.2824e-01,  4.9139e-01,\n",
      "          9.2378e-01, -6.6123e-01,  9.3111e-01,  9.6157e-01,  6.8816e-01,\n",
      "         -6.7706e-01,  1.2696e-01, -9.8735e-01, -1.3884e-01, -4.2742e-01,\n",
      "         -9.9151e-01,  3.1577e-01, -7.9245e-01,  1.1233e-01,  2.5441e-02,\n",
      "         -9.0001e-01,  2.9572e-01,  9.9972e-01,  2.9507e-02,  9.4610e-02,\n",
      "         -2.3375e-01, -1.0000e+00,  2.1042e-01, -8.8451e-01,  4.4717e-01,\n",
      "          2.8363e-01,  2.3094e-01,  1.6396e-01,  4.6484e-01,  4.1578e-01,\n",
      "          1.0880e-01, -6.9020e-02,  1.0251e-01, -1.8506e-01, -5.5142e-01,\n",
      "         -5.4772e-01,  3.2660e-01, -4.7119e-01, -9.0760e-01,  4.0407e-01,\n",
      "          8.4486e-03, -5.0541e-02, -2.2187e-01, -3.5096e-02, -1.5909e-01,\n",
      "          8.6302e-01,  2.0347e-01,  2.2685e-01, -8.1223e-01,  8.9115e-02,\n",
      "          1.9976e-01, -5.3915e-01,  1.0000e+00, -5.2943e-01, -9.8107e-01,\n",
      "          1.2214e-01,  1.7052e-01,  4.4200e-01,  2.9078e-01, -2.2429e-01,\n",
      "         -1.0000e+00,  4.5435e-01, -1.0752e-01, -9.8928e-01,  1.9557e-01,\n",
      "          3.7944e-01, -1.3400e-01, -3.7233e-01,  4.8776e-01, -2.7150e-01,\n",
      "         -2.9951e-01, -2.5372e-01, -3.0055e-01, -1.6436e-01, -2.0432e-01,\n",
      "          4.5899e-02, -1.5984e-01, -6.8892e-02, -2.8614e-01,  2.0454e-01,\n",
      "         -4.2329e-01, -6.0077e-01,  2.4896e-01, -2.7211e-01,  6.3257e-01,\n",
      "          2.8514e-01, -2.4841e-01,  3.5746e-01, -9.6110e-01,  6.6496e-01,\n",
      "         -2.5975e-01, -9.8228e-01, -5.0323e-01, -9.8950e-01,  7.1751e-01,\n",
      "          3.3974e-02, -1.6794e-01,  9.6757e-01,  3.7200e-01,  2.8462e-01,\n",
      "         -2.2425e-02, -3.7636e-01, -1.0000e+00, -5.5433e-01, -1.5097e-01,\n",
      "          1.1861e-01, -2.0500e-01, -9.7818e-01, -9.4408e-01,  6.0784e-01,\n",
      "          9.5020e-01,  9.3145e-02,  9.9944e-01, -3.1656e-01,  9.3937e-01,\n",
      "          1.0257e-02, -3.0524e-01,  1.3859e-01, -4.5599e-01,  4.7351e-01,\n",
      "          3.6347e-01, -7.3368e-01,  1.5255e-01, -4.4811e-02,  2.6415e-01,\n",
      "         -2.6249e-01, -2.5647e-01, -1.9468e-01, -9.3597e-01, -3.6595e-01,\n",
      "          9.4583e-01, -6.5554e-02, -4.0124e-01,  5.3403e-01, -2.8004e-01,\n",
      "         -3.8514e-01,  8.5284e-01,  3.8798e-01,  3.1723e-01, -9.4208e-02,\n",
      "          4.4104e-01, -1.2676e-01,  5.5534e-01, -8.1261e-01,  1.4222e-02,\n",
      "          4.3945e-01, -2.3209e-01, -1.5538e-01, -9.8027e-01, -3.1189e-01,\n",
      "          4.7667e-01,  9.8781e-01,  7.5918e-01,  2.3170e-01,  5.0506e-01,\n",
      "         -1.5959e-01,  5.4271e-01, -9.5258e-01,  9.7944e-01, -2.4452e-01,\n",
      "          2.3859e-01,  4.9278e-01,  1.4287e-01, -8.5605e-01, -3.0050e-01,\n",
      "          8.2882e-01, -5.1995e-01, -8.5260e-01,  8.5934e-02, -4.4540e-01,\n",
      "         -3.9270e-01, -1.5281e-01,  5.1311e-01, -2.6343e-01, -3.6877e-01,\n",
      "          2.9660e-02,  9.0136e-01,  9.7166e-01,  8.2276e-01, -3.7987e-01,\n",
      "          6.0987e-01, -9.1858e-01, -4.1369e-01,  8.5552e-02,  2.2591e-01,\n",
      "          1.2908e-01,  9.9388e-01, -5.4387e-02, -1.7653e-01, -9.3923e-01,\n",
      "         -9.8710e-01, -5.6532e-02, -9.0781e-01, -3.8273e-02, -6.4293e-01,\n",
      "          4.0239e-01,  4.5160e-01,  1.6601e-01,  4.0908e-01, -9.9176e-01,\n",
      "         -7.7417e-01,  3.3418e-01, -3.0262e-01,  4.0393e-01, -2.2568e-01,\n",
      "          3.4403e-01,  6.5064e-01, -5.2289e-01,  7.9253e-01,  8.5089e-01,\n",
      "         -9.9031e-02, -6.6476e-01,  8.4785e-01, -2.3960e-01,  8.9095e-01,\n",
      "         -6.0081e-01,  9.8772e-01,  5.2637e-01,  7.1539e-01, -9.4382e-01,\n",
      "         -2.6945e-02, -8.9317e-01, -2.5833e-01, -5.0681e-02, -4.4805e-01,\n",
      "          4.6222e-01,  4.8956e-01,  3.5595e-01,  7.6262e-01, -6.2524e-01,\n",
      "          9.9796e-01, -3.9808e-01, -9.5422e-01,  2.1079e-01, -2.4096e-01,\n",
      "         -9.8628e-01,  4.0971e-01,  2.0490e-01, -4.3229e-01, -3.5297e-01,\n",
      "         -4.6596e-01, -9.5645e-01,  9.1401e-01,  7.0786e-02,  9.9075e-01,\n",
      "          6.7400e-03, -9.3715e-01, -4.8485e-01, -9.1042e-01, -2.1888e-01,\n",
      "         -1.3952e-01,  3.0265e-01, -2.1077e-01, -9.6109e-01,  4.5869e-01,\n",
      "          4.8165e-01,  4.8832e-01, -1.0897e-01,  9.9831e-01,  9.9998e-01,\n",
      "          9.7482e-01,  8.9820e-01,  9.0949e-01, -9.8556e-01, -2.5522e-01,\n",
      "          9.9999e-01, -9.0774e-01, -1.0000e+00, -9.2744e-01, -5.5746e-01,\n",
      "          2.4192e-01, -1.0000e+00, -3.7209e-02,  5.3762e-02, -9.2917e-01,\n",
      "         -1.7546e-02,  9.8157e-01,  9.9313e-01, -1.0000e+00,  8.6012e-01,\n",
      "          9.4574e-01, -5.6222e-01,  7.5814e-01, -2.3953e-01,  9.7340e-01,\n",
      "          3.6503e-01,  2.8978e-01, -2.3095e-01,  3.4518e-01, -4.6773e-01,\n",
      "         -8.5951e-01,  4.7867e-02, -9.3612e-02,  9.0046e-01,  9.4787e-02,\n",
      "         -7.3987e-01, -9.4770e-01,  1.3531e-01, -1.3985e-01, -2.2744e-01,\n",
      "         -9.6028e-01, -7.6159e-02,  2.8726e-01,  7.1045e-01,  8.1054e-02,\n",
      "          2.3858e-01, -7.8597e-01,  1.9432e-01, -6.5189e-01,  4.5484e-01,\n",
      "          5.9485e-01, -9.5303e-01, -6.7571e-01, -1.6829e-01, -2.6858e-01,\n",
      "         -1.8851e-02, -9.3974e-01,  9.7244e-01, -4.1418e-01,  5.5397e-01,\n",
      "          1.0000e+00,  6.7241e-02, -9.0357e-01,  3.8333e-01,  1.7075e-01,\n",
      "          9.1030e-02,  1.0000e+00,  6.6256e-01, -9.7822e-01, -4.7003e-01,\n",
      "          4.8857e-01, -4.7954e-01, -4.4943e-01,  9.9902e-01, -2.4455e-01,\n",
      "          2.7924e-02,  2.6862e-01,  9.7364e-01, -9.9015e-01,  8.2752e-01,\n",
      "         -9.1195e-01, -9.6930e-01,  9.6406e-01,  9.3691e-01, -3.6014e-01,\n",
      "         -7.0250e-01,  1.0417e-01, -1.8180e-01,  2.3008e-01, -9.6799e-01,\n",
      "          6.9217e-01,  4.2967e-01, -1.0165e-01,  9.0773e-01, -8.7691e-01,\n",
      "         -4.3018e-01,  3.5562e-01, -2.2115e-01,  1.6860e-01,  4.9965e-01,\n",
      "          5.1945e-01, -1.8539e-01,  1.0172e-01, -2.1061e-01,  3.3852e-03,\n",
      "         -9.7327e-01,  1.3324e-01,  1.0000e+00, -7.5041e-02,  3.6617e-02,\n",
      "         -3.9262e-01, -6.5352e-03, -2.7287e-01,  3.9574e-01,  3.9382e-01,\n",
      "         -3.0707e-01, -8.4807e-01,  3.8420e-01, -9.5470e-01, -9.8489e-01,\n",
      "          7.6798e-01,  1.4348e-01, -2.8546e-01,  9.9994e-01,  4.1162e-01,\n",
      "          1.6070e-01,  1.2186e-01,  8.2578e-01, -5.7293e-02,  5.6674e-01,\n",
      "          1.8886e-01,  9.7533e-01, -1.4569e-01,  4.8805e-01,  8.5755e-01,\n",
      "         -4.0104e-01, -2.8342e-01, -6.4264e-01, -5.3019e-03, -9.0426e-01,\n",
      "          6.8657e-03, -9.5985e-01,  9.6273e-01,  5.4771e-01,  2.7272e-01,\n",
      "          1.1672e-01,  1.8696e-01,  1.0000e+00,  1.3563e-01,  5.7521e-01,\n",
      "         -5.4665e-01,  8.8256e-01, -9.7980e-01, -8.2186e-01, -3.9412e-01,\n",
      "          6.9253e-02, -2.4739e-01, -2.4468e-01,  2.2320e-01, -9.7092e-01,\n",
      "          1.6613e-01,  2.7089e-01, -9.8789e-01, -9.9264e-01,  1.4982e-01,\n",
      "          7.7813e-01,  7.5834e-02, -7.5816e-01, -6.9681e-01, -5.4449e-01,\n",
      "          4.2848e-01, -2.6814e-01, -9.4467e-01,  2.8673e-01, -1.7200e-01,\n",
      "          4.9967e-01, -1.7855e-01,  4.6067e-01,  1.0443e-01,  8.0958e-01,\n",
      "          3.0960e-01,  1.1103e-01,  2.0682e-02, -7.7644e-01,  7.9258e-01,\n",
      "         -8.3320e-01, -4.5220e-01, -9.7233e-02,  1.0000e+00, -4.7325e-01,\n",
      "          2.6074e-01,  7.4868e-01,  7.9047e-01, -1.2065e-01,  1.6631e-01,\n",
      "          4.3811e-01,  1.9812e-01, -1.2818e-01, -1.4429e-01, -7.9943e-01,\n",
      "         -3.4779e-01,  5.3907e-01, -2.8876e-01, -1.2784e-01,  7.8907e-01,\n",
      "          2.7644e-01,  1.2553e-01,  9.6569e-02, -8.6027e-04,  9.9958e-01,\n",
      "         -2.4824e-01, -1.9148e-01, -4.8891e-01,  3.5458e-02, -3.5075e-01,\n",
      "         -4.9195e-01,  1.0000e+00,  3.1492e-01,  1.3190e-01, -9.9190e-01,\n",
      "         -2.3020e-01, -9.1818e-01,  9.9990e-01,  8.1607e-01, -8.3672e-01,\n",
      "          5.7710e-01,  5.4205e-01, -6.0926e-02,  8.1453e-01, -1.8440e-01,\n",
      "         -1.1731e-01,  2.2679e-01,  4.9845e-02,  9.6623e-01, -4.2748e-01,\n",
      "         -9.6389e-01, -5.0603e-01,  3.6750e-01, -9.6186e-01,  9.8817e-01,\n",
      "         -4.9461e-01, -1.6667e-01, -2.8972e-01,  5.0912e-02,  7.1987e-01,\n",
      "          3.2723e-02, -9.8163e-01, -1.8460e-01,  2.4921e-02,  9.6903e-01,\n",
      "          1.4459e-01, -5.1089e-01, -9.1297e-01,  1.9776e-01,  3.1596e-01,\n",
      "         -3.4459e-01, -9.5039e-01,  9.7347e-01, -9.8541e-01,  5.6550e-01,\n",
      "          1.0000e+00,  1.6803e-01, -5.2763e-01,  1.7335e-01, -3.5292e-01,\n",
      "          2.4425e-01, -1.6082e-01,  5.5743e-01, -9.5780e-01, -2.7111e-01,\n",
      "         -1.5769e-01,  3.2735e-01, -1.9443e-01,  5.6069e-02,  7.1469e-01,\n",
      "          1.4012e-01, -4.5109e-01, -5.5167e-01, -5.8712e-02,  4.0066e-01,\n",
      "          8.0510e-01, -2.4921e-01, -1.1596e-01,  8.1007e-02, -6.8707e-02,\n",
      "         -9.5142e-01, -3.0857e-01, -3.3517e-01, -9.9849e-01,  5.8358e-01,\n",
      "         -1.0000e+00, -1.7750e-02, -3.8238e-01, -2.0135e-01,  8.2339e-01,\n",
      "          7.3314e-02,  2.2555e-01, -7.3748e-01, -1.5620e-01,  7.2231e-01,\n",
      "          7.8650e-01, -2.6355e-01, -2.3780e-01, -7.5355e-01,  2.1576e-01,\n",
      "          3.3272e-03,  3.2797e-01, -1.6444e-01,  7.0653e-01, -1.6586e-01,\n",
      "          1.0000e+00,  3.6853e-02, -5.5209e-01, -9.7956e-01,  2.1084e-01,\n",
      "         -2.2580e-01,  9.9999e-01, -9.0543e-01, -9.5194e-01,  3.5803e-01,\n",
      "         -5.4486e-01, -8.3170e-01,  2.0722e-01, -1.1187e-01, -7.1528e-01,\n",
      "         -6.5816e-01,  9.5457e-01,  8.6705e-01, -4.6680e-01,  4.5275e-01,\n",
      "         -2.8203e-01, -5.2516e-01,  2.4673e-02,  2.3666e-01,  9.8751e-01,\n",
      "          1.6960e-01,  9.1254e-01,  3.8521e-01, -5.2774e-02,  9.6522e-01,\n",
      "          2.1477e-01,  6.8869e-01,  1.3419e-01,  1.0000e+00,  2.5379e-01,\n",
      "         -9.1662e-01,  1.8445e-01, -9.8539e-01, -1.7379e-01, -9.5886e-01,\n",
      "          2.4864e-01,  1.0564e-01,  8.8982e-01, -1.9566e-01,  9.6709e-01,\n",
      "         -1.6387e-01,  6.2168e-02, -3.2527e-01,  2.9287e-01,  3.6544e-01,\n",
      "         -9.2183e-01, -9.8603e-01, -9.8862e-01,  5.1165e-01, -4.1985e-01,\n",
      "         -5.2828e-02,  2.2914e-01,  1.5558e-01,  3.7237e-01,  4.2995e-01,\n",
      "         -1.0000e+00,  9.4092e-01,  3.5116e-01,  2.6950e-01,  9.6724e-01,\n",
      "          5.0886e-01,  4.4842e-01,  1.9517e-01, -9.8747e-01, -9.7901e-01,\n",
      "         -3.3062e-01, -2.0128e-01,  7.6804e-01,  6.3693e-01,  8.5249e-01,\n",
      "          4.2277e-01, -4.5014e-01, -8.7409e-02,  2.3122e-01, -4.1254e-01,\n",
      "         -9.9309e-01,  3.7523e-01, -9.7383e-02, -9.7192e-01,  9.5663e-01,\n",
      "         -3.8314e-01, -9.1959e-02,  3.2168e-01, -2.1238e-01,  9.5619e-01,\n",
      "          8.0648e-01,  4.8650e-01,  1.0678e-01,  4.6444e-01,  9.2019e-01,\n",
      "          9.4756e-01,  9.8960e-01, -3.1468e-01,  7.9191e-01, -2.3371e-01,\n",
      "          4.0808e-01,  4.0834e-01, -9.2244e-01,  8.3447e-02,  1.1936e-01,\n",
      "         -1.6753e-01,  2.4999e-01, -1.7787e-01, -9.7539e-01,  4.0304e-01,\n",
      "         -2.3984e-01,  4.3153e-01, -4.0423e-01,  1.3859e-01, -3.7546e-01,\n",
      "         -1.1792e-01, -6.6992e-01, -5.7544e-01,  5.6841e-01,  2.7722e-01,\n",
      "          9.1280e-01,  5.2105e-01, -2.5549e-02, -7.2245e-01, -2.0173e-01,\n",
      "         -3.1763e-02, -9.1190e-01,  9.3950e-01, -5.4791e-02,  2.3775e-01,\n",
      "         -9.2643e-02, -5.8986e-02,  5.5153e-01, -1.7753e-01, -3.7412e-01,\n",
      "         -3.0713e-01, -7.3179e-01,  8.9369e-01, -1.9860e-01, -4.5436e-01,\n",
      "         -5.1354e-01,  7.0488e-01,  2.6542e-01,  9.9715e-01, -1.5817e-01,\n",
      "         -5.1730e-01, -2.4476e-01, -2.4036e-01,  2.6017e-01, -2.8945e-01,\n",
      "         -1.0000e+00,  4.2029e-01, -1.4138e-01,  2.0229e-01, -2.8328e-01,\n",
      "          1.8165e-01, -3.3790e-01, -9.8112e-01, -1.1746e-01,  4.8850e-02,\n",
      "          9.2831e-02, -4.8529e-01, -6.1237e-01,  5.2537e-01, -2.1332e-01,\n",
      "          7.9100e-01,  8.9052e-01,  2.9391e-02,  5.8748e-01,  5.7159e-01,\n",
      "          8.0725e-02, -6.6481e-01,  9.1761e-01]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"hidden_rep: \\n {}\".format(hidden_rep))\n",
    "print(\"cls_head: \\n {}\".format(cls_head))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_rep shape: \n",
      " torch.Size([1, 7, 768])\n",
      "cls_head shape: \n",
      " torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"hidden_rep shape: \\n {}\".format(hidden_rep.size()))\n",
    "print(\"cls_head shape: \\n {}\".format(cls_head.size()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_rep num elementi: \n",
      " 5376\n",
      "cls_head num elementi: \n",
      " 768\n"
     ]
    }
   ],
   "source": [
    "print(\"hidden_rep num elementi: \\n {}\".format(torch.numel(hidden_rep)))\n",
    "print(\"cls_head num elementi: \\n {}\".format(torch.numel(cls_head)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "E' possibile ottenere anche i valori delle singole rappresentazioni da hidden_rep"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#hidden_rep[0][0] #R_CLS\n",
    "#hidden_rep[0][1] #R_I\n",
    "#hidden_rep[0][2] #R_love"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Possiamo anche estrarre gli incorporamenti da tutti gli strati dell'encoder di BERT? Sì!\n",
    "Vediamo come, dobbiamo in pratica rifare da capo tutto!\n",
    "Per prima cosa quando scarichiamo il modello bert dobbiamo impostare a true un valore di flag\n",
    "chiamato output_hidden_states = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "sentence = \"I love Paris\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'paris']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora aggiungiamo i token speciali, quello  di classificazione [CLS] e quello di fine frase [SEP]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'paris', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['[CLS]']+tokens+['[SEP]']\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La lista dei tokens è 5, ma supponiamo che dobbiamo avere un numero di tokens fisso diciamo che la lista\n",
    "deve essere lunga 7. Per raggiungere questo obbiettivo possiamo pensare di riempirla con un token speciale\n",
    "che come unico scopo sia solo quello di riempire la lista dei token, [PAD]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "tokens = tokens+ ['[PAD]']+ ['[PAD]']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'paris', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A questo punto dobbiamo dire al nostro modello quali sono i token reali e quali no ([PAD]),\n",
    "per fare questo dobbiamo definire un oggetto chiamato attention mask (da non confondere per la matrice di attenzione)\n",
    "Attention mask assegna 1 ai token diversi da [PAD] e 0 a [PAD]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "attention_mask = [1 if token != '[PAD]' else 0 for token in tokens]\n",
    "print(attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora facciamo un'altro passo che nella teoria non viene detto.\n",
    "I tokens non possono rimanere parole, ma devono essere convertite in numeri perchè gli algoritmi lavorano\n",
    "su questo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2293, 3000, 102, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dall'output, possiamo osservare che ogni token è mappato a un ID token univoco.\n",
    "Il prossimo passo è convertite la maschera di attenzione e la lista tokens_id in tensori con pytorch\n",
    "si consiglia di portarli alla gpu\n",
    "Si raccomanda di fare unsqueeze(0) per aumentare le dimensioni del tensore, in quanto la semplice lista\n",
    "non permette di rappresentare un vettore riga che è caratterizzato da 2 dimensioni (riga, colonna),\n",
    "senza unsqueeze(0) non avremmo la rappresentazione algebrica in quanto sarebbe su un unica dimensione e rappresentebbe\n",
    "solo il numero di elementi che compongono il tensore che in questo caso sarebbe solo 7\n",
    "hidden_states è una lista contenete dei tensori, sono 13 tensori di dimensione (1,7,768), il\n",
    "primo tensore è il embedding finale che si costruisce dalla somma di segment embedding, positional embedding e token\n",
    "embedding. Gli altri 12 tensori sono le varie rappresentazioni intermedie calcolate dai vari encoder dentro il bert, poichè\n",
    "questo è un bart base ci sono 12 encoder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(tokens_id).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poiché durante la definizione del modello abbiamo impostato  output_hidden_states = True\n",
    "per ottenere gli incorporamenti da tutti i layer di encoder, ora il modello restituisce una tupla di output\n",
    "con tre valori, come mostrato nel codice seguente. Vediamo che restituisce 3 oggetti, analizziamoli.\n",
    "pooler_output è un tensore di dimensione (1, 768) che rappresenta la rappresentazione finale del token CLS, R_CLS,\n",
    "viene ulteriormente elaborato da una funzione di attivazione lineare e tanh.\n",
    "last_hidden_state è un tensore di dimensione (1, 7, 768) che contiene le rappresentazioni finali di tutti i token\n",
    "presenti  nella lista dei tokens.\n",
    "hidden_states è una lista che contiene"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "last_hidden_state, pooler_output, hidden_states = \\\n",
    "model(token_ids, attention_mask = attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state: \n",
      " tensor([[[-0.0719,  0.2163,  0.0047,  ..., -0.5865,  0.2262,  0.1981],\n",
      "         [ 0.2236,  0.6536, -0.2294,  ..., -0.3547,  0.5517, -0.2367],\n",
      "         [ 1.0410,  0.7755,  1.0335,  ..., -0.5621,  0.5218, -0.0852],\n",
      "         ...,\n",
      "         [ 0.6156,  0.1036, -0.1875,  ..., -0.3799, -0.7008, -0.3500],\n",
      "         [ 0.0791,  0.4287,  0.4147,  ..., -0.2417,  0.2403,  0.0378],\n",
      "         [-0.0165,  0.2459,  0.4566,  ..., -0.2179,  0.1876,  0.0228]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "pooler_output: \n",
      " tensor([[-9.0660e-01, -3.4189e-01, -3.3729e-01,  7.7140e-01,  6.0975e-02,\n",
      "         -1.0525e-01,  9.0143e-01,  2.5822e-01, -2.7881e-01, -9.9997e-01,\n",
      "         -1.0322e-01,  7.4772e-01,  9.8521e-01,  5.9799e-02,  9.4447e-01,\n",
      "         -5.9859e-01, -2.0539e-01, -5.7386e-01,  3.7684e-01, -7.5183e-01,\n",
      "          6.6604e-01,  9.9584e-01,  4.2231e-01,  2.2824e-01,  4.9139e-01,\n",
      "          9.2378e-01, -6.6123e-01,  9.3111e-01,  9.6157e-01,  6.8816e-01,\n",
      "         -6.7706e-01,  1.2696e-01, -9.8735e-01, -1.3884e-01, -4.2742e-01,\n",
      "         -9.9151e-01,  3.1577e-01, -7.9245e-01,  1.1233e-01,  2.5441e-02,\n",
      "         -9.0001e-01,  2.9572e-01,  9.9972e-01,  2.9507e-02,  9.4610e-02,\n",
      "         -2.3375e-01, -1.0000e+00,  2.1042e-01, -8.8451e-01,  4.4717e-01,\n",
      "          2.8363e-01,  2.3094e-01,  1.6396e-01,  4.6484e-01,  4.1578e-01,\n",
      "          1.0880e-01, -6.9020e-02,  1.0251e-01, -1.8506e-01, -5.5142e-01,\n",
      "         -5.4772e-01,  3.2660e-01, -4.7119e-01, -9.0760e-01,  4.0407e-01,\n",
      "          8.4486e-03, -5.0541e-02, -2.2187e-01, -3.5096e-02, -1.5909e-01,\n",
      "          8.6302e-01,  2.0347e-01,  2.2685e-01, -8.1223e-01,  8.9115e-02,\n",
      "          1.9976e-01, -5.3915e-01,  1.0000e+00, -5.2943e-01, -9.8107e-01,\n",
      "          1.2214e-01,  1.7052e-01,  4.4200e-01,  2.9078e-01, -2.2429e-01,\n",
      "         -1.0000e+00,  4.5435e-01, -1.0752e-01, -9.8928e-01,  1.9557e-01,\n",
      "          3.7944e-01, -1.3400e-01, -3.7233e-01,  4.8776e-01, -2.7150e-01,\n",
      "         -2.9951e-01, -2.5372e-01, -3.0055e-01, -1.6436e-01, -2.0432e-01,\n",
      "          4.5899e-02, -1.5984e-01, -6.8892e-02, -2.8614e-01,  2.0454e-01,\n",
      "         -4.2329e-01, -6.0077e-01,  2.4896e-01, -2.7211e-01,  6.3257e-01,\n",
      "          2.8514e-01, -2.4841e-01,  3.5746e-01, -9.6110e-01,  6.6496e-01,\n",
      "         -2.5975e-01, -9.8228e-01, -5.0323e-01, -9.8950e-01,  7.1751e-01,\n",
      "          3.3974e-02, -1.6794e-01,  9.6757e-01,  3.7200e-01,  2.8462e-01,\n",
      "         -2.2425e-02, -3.7636e-01, -1.0000e+00, -5.5433e-01, -1.5097e-01,\n",
      "          1.1861e-01, -2.0500e-01, -9.7818e-01, -9.4408e-01,  6.0784e-01,\n",
      "          9.5020e-01,  9.3145e-02,  9.9944e-01, -3.1656e-01,  9.3937e-01,\n",
      "          1.0257e-02, -3.0524e-01,  1.3859e-01, -4.5599e-01,  4.7351e-01,\n",
      "          3.6347e-01, -7.3368e-01,  1.5255e-01, -4.4811e-02,  2.6415e-01,\n",
      "         -2.6249e-01, -2.5647e-01, -1.9468e-01, -9.3597e-01, -3.6595e-01,\n",
      "          9.4583e-01, -6.5554e-02, -4.0124e-01,  5.3403e-01, -2.8004e-01,\n",
      "         -3.8514e-01,  8.5284e-01,  3.8798e-01,  3.1723e-01, -9.4208e-02,\n",
      "          4.4104e-01, -1.2676e-01,  5.5534e-01, -8.1261e-01,  1.4222e-02,\n",
      "          4.3945e-01, -2.3209e-01, -1.5538e-01, -9.8027e-01, -3.1189e-01,\n",
      "          4.7667e-01,  9.8781e-01,  7.5918e-01,  2.3170e-01,  5.0506e-01,\n",
      "         -1.5959e-01,  5.4271e-01, -9.5258e-01,  9.7944e-01, -2.4452e-01,\n",
      "          2.3859e-01,  4.9278e-01,  1.4287e-01, -8.5605e-01, -3.0050e-01,\n",
      "          8.2882e-01, -5.1995e-01, -8.5260e-01,  8.5934e-02, -4.4540e-01,\n",
      "         -3.9270e-01, -1.5281e-01,  5.1311e-01, -2.6343e-01, -3.6877e-01,\n",
      "          2.9660e-02,  9.0136e-01,  9.7166e-01,  8.2276e-01, -3.7987e-01,\n",
      "          6.0987e-01, -9.1858e-01, -4.1369e-01,  8.5552e-02,  2.2591e-01,\n",
      "          1.2908e-01,  9.9388e-01, -5.4387e-02, -1.7653e-01, -9.3923e-01,\n",
      "         -9.8710e-01, -5.6532e-02, -9.0781e-01, -3.8273e-02, -6.4293e-01,\n",
      "          4.0239e-01,  4.5160e-01,  1.6601e-01,  4.0908e-01, -9.9176e-01,\n",
      "         -7.7417e-01,  3.3418e-01, -3.0262e-01,  4.0393e-01, -2.2568e-01,\n",
      "          3.4403e-01,  6.5064e-01, -5.2289e-01,  7.9253e-01,  8.5089e-01,\n",
      "         -9.9031e-02, -6.6476e-01,  8.4785e-01, -2.3960e-01,  8.9095e-01,\n",
      "         -6.0081e-01,  9.8772e-01,  5.2637e-01,  7.1539e-01, -9.4382e-01,\n",
      "         -2.6945e-02, -8.9317e-01, -2.5833e-01, -5.0681e-02, -4.4805e-01,\n",
      "          4.6222e-01,  4.8956e-01,  3.5595e-01,  7.6262e-01, -6.2524e-01,\n",
      "          9.9796e-01, -3.9808e-01, -9.5422e-01,  2.1079e-01, -2.4096e-01,\n",
      "         -9.8628e-01,  4.0971e-01,  2.0490e-01, -4.3229e-01, -3.5297e-01,\n",
      "         -4.6596e-01, -9.5645e-01,  9.1401e-01,  7.0786e-02,  9.9075e-01,\n",
      "          6.7400e-03, -9.3715e-01, -4.8485e-01, -9.1042e-01, -2.1888e-01,\n",
      "         -1.3952e-01,  3.0265e-01, -2.1077e-01, -9.6109e-01,  4.5869e-01,\n",
      "          4.8165e-01,  4.8832e-01, -1.0897e-01,  9.9831e-01,  9.9998e-01,\n",
      "          9.7482e-01,  8.9820e-01,  9.0949e-01, -9.8556e-01, -2.5522e-01,\n",
      "          9.9999e-01, -9.0774e-01, -1.0000e+00, -9.2744e-01, -5.5746e-01,\n",
      "          2.4192e-01, -1.0000e+00, -3.7209e-02,  5.3762e-02, -9.2917e-01,\n",
      "         -1.7546e-02,  9.8157e-01,  9.9313e-01, -1.0000e+00,  8.6012e-01,\n",
      "          9.4574e-01, -5.6222e-01,  7.5814e-01, -2.3953e-01,  9.7340e-01,\n",
      "          3.6503e-01,  2.8978e-01, -2.3095e-01,  3.4518e-01, -4.6773e-01,\n",
      "         -8.5951e-01,  4.7867e-02, -9.3612e-02,  9.0046e-01,  9.4787e-02,\n",
      "         -7.3987e-01, -9.4770e-01,  1.3531e-01, -1.3985e-01, -2.2744e-01,\n",
      "         -9.6028e-01, -7.6159e-02,  2.8726e-01,  7.1045e-01,  8.1054e-02,\n",
      "          2.3858e-01, -7.8597e-01,  1.9432e-01, -6.5189e-01,  4.5484e-01,\n",
      "          5.9485e-01, -9.5303e-01, -6.7571e-01, -1.6829e-01, -2.6858e-01,\n",
      "         -1.8851e-02, -9.3974e-01,  9.7244e-01, -4.1418e-01,  5.5397e-01,\n",
      "          1.0000e+00,  6.7241e-02, -9.0357e-01,  3.8333e-01,  1.7075e-01,\n",
      "          9.1030e-02,  1.0000e+00,  6.6256e-01, -9.7822e-01, -4.7003e-01,\n",
      "          4.8857e-01, -4.7954e-01, -4.4943e-01,  9.9902e-01, -2.4455e-01,\n",
      "          2.7924e-02,  2.6862e-01,  9.7364e-01, -9.9015e-01,  8.2752e-01,\n",
      "         -9.1195e-01, -9.6930e-01,  9.6406e-01,  9.3691e-01, -3.6014e-01,\n",
      "         -7.0250e-01,  1.0417e-01, -1.8180e-01,  2.3008e-01, -9.6799e-01,\n",
      "          6.9217e-01,  4.2967e-01, -1.0165e-01,  9.0773e-01, -8.7691e-01,\n",
      "         -4.3018e-01,  3.5562e-01, -2.2115e-01,  1.6860e-01,  4.9965e-01,\n",
      "          5.1945e-01, -1.8539e-01,  1.0172e-01, -2.1061e-01,  3.3852e-03,\n",
      "         -9.7327e-01,  1.3324e-01,  1.0000e+00, -7.5041e-02,  3.6617e-02,\n",
      "         -3.9262e-01, -6.5352e-03, -2.7287e-01,  3.9574e-01,  3.9382e-01,\n",
      "         -3.0707e-01, -8.4807e-01,  3.8420e-01, -9.5470e-01, -9.8489e-01,\n",
      "          7.6798e-01,  1.4348e-01, -2.8546e-01,  9.9994e-01,  4.1162e-01,\n",
      "          1.6070e-01,  1.2186e-01,  8.2578e-01, -5.7293e-02,  5.6674e-01,\n",
      "          1.8886e-01,  9.7533e-01, -1.4569e-01,  4.8805e-01,  8.5755e-01,\n",
      "         -4.0104e-01, -2.8342e-01, -6.4264e-01, -5.3019e-03, -9.0426e-01,\n",
      "          6.8657e-03, -9.5985e-01,  9.6273e-01,  5.4771e-01,  2.7272e-01,\n",
      "          1.1672e-01,  1.8696e-01,  1.0000e+00,  1.3563e-01,  5.7521e-01,\n",
      "         -5.4665e-01,  8.8256e-01, -9.7980e-01, -8.2186e-01, -3.9412e-01,\n",
      "          6.9253e-02, -2.4739e-01, -2.4468e-01,  2.2320e-01, -9.7092e-01,\n",
      "          1.6613e-01,  2.7089e-01, -9.8789e-01, -9.9264e-01,  1.4982e-01,\n",
      "          7.7813e-01,  7.5834e-02, -7.5816e-01, -6.9681e-01, -5.4449e-01,\n",
      "          4.2848e-01, -2.6814e-01, -9.4467e-01,  2.8673e-01, -1.7200e-01,\n",
      "          4.9967e-01, -1.7855e-01,  4.6067e-01,  1.0443e-01,  8.0958e-01,\n",
      "          3.0960e-01,  1.1103e-01,  2.0682e-02, -7.7644e-01,  7.9258e-01,\n",
      "         -8.3320e-01, -4.5220e-01, -9.7233e-02,  1.0000e+00, -4.7325e-01,\n",
      "          2.6074e-01,  7.4868e-01,  7.9047e-01, -1.2065e-01,  1.6631e-01,\n",
      "          4.3811e-01,  1.9812e-01, -1.2818e-01, -1.4429e-01, -7.9943e-01,\n",
      "         -3.4779e-01,  5.3907e-01, -2.8876e-01, -1.2784e-01,  7.8907e-01,\n",
      "          2.7644e-01,  1.2553e-01,  9.6569e-02, -8.6027e-04,  9.9958e-01,\n",
      "         -2.4824e-01, -1.9148e-01, -4.8891e-01,  3.5458e-02, -3.5075e-01,\n",
      "         -4.9195e-01,  1.0000e+00,  3.1492e-01,  1.3190e-01, -9.9190e-01,\n",
      "         -2.3020e-01, -9.1818e-01,  9.9990e-01,  8.1607e-01, -8.3672e-01,\n",
      "          5.7710e-01,  5.4205e-01, -6.0926e-02,  8.1453e-01, -1.8440e-01,\n",
      "         -1.1731e-01,  2.2679e-01,  4.9845e-02,  9.6623e-01, -4.2748e-01,\n",
      "         -9.6389e-01, -5.0603e-01,  3.6750e-01, -9.6186e-01,  9.8817e-01,\n",
      "         -4.9461e-01, -1.6667e-01, -2.8972e-01,  5.0912e-02,  7.1987e-01,\n",
      "          3.2723e-02, -9.8163e-01, -1.8460e-01,  2.4921e-02,  9.6903e-01,\n",
      "          1.4459e-01, -5.1089e-01, -9.1297e-01,  1.9776e-01,  3.1596e-01,\n",
      "         -3.4459e-01, -9.5039e-01,  9.7347e-01, -9.8541e-01,  5.6550e-01,\n",
      "          1.0000e+00,  1.6803e-01, -5.2763e-01,  1.7335e-01, -3.5292e-01,\n",
      "          2.4425e-01, -1.6082e-01,  5.5743e-01, -9.5780e-01, -2.7111e-01,\n",
      "         -1.5769e-01,  3.2735e-01, -1.9443e-01,  5.6069e-02,  7.1469e-01,\n",
      "          1.4012e-01, -4.5109e-01, -5.5167e-01, -5.8712e-02,  4.0066e-01,\n",
      "          8.0510e-01, -2.4921e-01, -1.1596e-01,  8.1007e-02, -6.8707e-02,\n",
      "         -9.5142e-01, -3.0857e-01, -3.3517e-01, -9.9849e-01,  5.8358e-01,\n",
      "         -1.0000e+00, -1.7750e-02, -3.8238e-01, -2.0135e-01,  8.2339e-01,\n",
      "          7.3314e-02,  2.2555e-01, -7.3748e-01, -1.5620e-01,  7.2231e-01,\n",
      "          7.8650e-01, -2.6355e-01, -2.3780e-01, -7.5355e-01,  2.1576e-01,\n",
      "          3.3272e-03,  3.2797e-01, -1.6444e-01,  7.0653e-01, -1.6586e-01,\n",
      "          1.0000e+00,  3.6853e-02, -5.5209e-01, -9.7956e-01,  2.1084e-01,\n",
      "         -2.2580e-01,  9.9999e-01, -9.0543e-01, -9.5194e-01,  3.5803e-01,\n",
      "         -5.4486e-01, -8.3170e-01,  2.0722e-01, -1.1187e-01, -7.1528e-01,\n",
      "         -6.5816e-01,  9.5457e-01,  8.6705e-01, -4.6680e-01,  4.5275e-01,\n",
      "         -2.8203e-01, -5.2516e-01,  2.4673e-02,  2.3666e-01,  9.8751e-01,\n",
      "          1.6960e-01,  9.1254e-01,  3.8521e-01, -5.2774e-02,  9.6522e-01,\n",
      "          2.1477e-01,  6.8869e-01,  1.3419e-01,  1.0000e+00,  2.5379e-01,\n",
      "         -9.1662e-01,  1.8445e-01, -9.8539e-01, -1.7379e-01, -9.5886e-01,\n",
      "          2.4864e-01,  1.0564e-01,  8.8982e-01, -1.9566e-01,  9.6709e-01,\n",
      "         -1.6387e-01,  6.2168e-02, -3.2527e-01,  2.9287e-01,  3.6544e-01,\n",
      "         -9.2183e-01, -9.8603e-01, -9.8862e-01,  5.1165e-01, -4.1985e-01,\n",
      "         -5.2828e-02,  2.2914e-01,  1.5558e-01,  3.7237e-01,  4.2995e-01,\n",
      "         -1.0000e+00,  9.4092e-01,  3.5116e-01,  2.6950e-01,  9.6724e-01,\n",
      "          5.0886e-01,  4.4842e-01,  1.9517e-01, -9.8747e-01, -9.7901e-01,\n",
      "         -3.3062e-01, -2.0128e-01,  7.6804e-01,  6.3693e-01,  8.5249e-01,\n",
      "          4.2277e-01, -4.5014e-01, -8.7409e-02,  2.3122e-01, -4.1254e-01,\n",
      "         -9.9309e-01,  3.7523e-01, -9.7383e-02, -9.7192e-01,  9.5663e-01,\n",
      "         -3.8314e-01, -9.1959e-02,  3.2168e-01, -2.1238e-01,  9.5619e-01,\n",
      "          8.0648e-01,  4.8650e-01,  1.0678e-01,  4.6444e-01,  9.2019e-01,\n",
      "          9.4756e-01,  9.8960e-01, -3.1468e-01,  7.9191e-01, -2.3371e-01,\n",
      "          4.0808e-01,  4.0834e-01, -9.2244e-01,  8.3447e-02,  1.1936e-01,\n",
      "         -1.6753e-01,  2.4999e-01, -1.7787e-01, -9.7539e-01,  4.0304e-01,\n",
      "         -2.3984e-01,  4.3153e-01, -4.0423e-01,  1.3859e-01, -3.7546e-01,\n",
      "         -1.1792e-01, -6.6992e-01, -5.7544e-01,  5.6841e-01,  2.7722e-01,\n",
      "          9.1280e-01,  5.2105e-01, -2.5549e-02, -7.2245e-01, -2.0173e-01,\n",
      "         -3.1763e-02, -9.1190e-01,  9.3950e-01, -5.4791e-02,  2.3775e-01,\n",
      "         -9.2643e-02, -5.8986e-02,  5.5153e-01, -1.7753e-01, -3.7412e-01,\n",
      "         -3.0713e-01, -7.3179e-01,  8.9369e-01, -1.9860e-01, -4.5436e-01,\n",
      "         -5.1354e-01,  7.0488e-01,  2.6542e-01,  9.9715e-01, -1.5817e-01,\n",
      "         -5.1730e-01, -2.4476e-01, -2.4036e-01,  2.6017e-01, -2.8945e-01,\n",
      "         -1.0000e+00,  4.2029e-01, -1.4138e-01,  2.0229e-01, -2.8328e-01,\n",
      "          1.8165e-01, -3.3790e-01, -9.8112e-01, -1.1746e-01,  4.8850e-02,\n",
      "          9.2831e-02, -4.8529e-01, -6.1237e-01,  5.2537e-01, -2.1332e-01,\n",
      "          7.9100e-01,  8.9052e-01,  2.9391e-02,  5.8748e-01,  5.7159e-01,\n",
      "          8.0725e-02, -6.6481e-01,  9.1761e-01]], grad_fn=<TanhBackward>)\n",
      "last_hidden_state: \n",
      " (tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
      "           3.8253e-02,  1.6400e-01],\n",
      "         [-3.4023e-04,  5.3974e-01, -2.8805e-01,  ...,  7.5731e-01,\n",
      "           8.9008e-01,  1.6575e-01],\n",
      "         [ 1.1558e+00,  8.5331e-02, -1.1208e-01,  ...,  4.3965e-01,\n",
      "           8.5903e-01, -3.2685e-01],\n",
      "         ...,\n",
      "         [-3.6430e-01, -1.6172e-01,  9.0174e-02,  ..., -1.7849e-01,\n",
      "           1.2818e-01, -4.5116e-02],\n",
      "         [ 1.6776e-01, -8.9038e-01, -3.1798e-01,  ...,  3.1737e-02,\n",
      "           6.4863e-02,  1.8418e-01],\n",
      "         [ 3.5675e-01, -8.7076e-01, -3.7621e-01,  ..., -7.9391e-02,\n",
      "          -1.0115e-01,  1.9312e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.1577,  0.0796, -0.0981,  ..., -0.0186,  0.1693,  0.0758],\n",
      "         [ 0.6816,  1.1077, -0.2971,  ...,  0.2517,  0.8083,  0.1772],\n",
      "         [ 1.6985,  0.8829,  0.1861,  ...,  0.4751,  0.8886,  0.0153],\n",
      "         ...,\n",
      "         [-0.0045,  0.3082, -0.0522,  ..., -0.2775,  0.6925,  0.1390],\n",
      "         [-0.0177, -0.7238,  0.2278,  ...,  0.4520,  0.4353, -0.1770],\n",
      "         [ 0.1643, -0.7343,  0.1229,  ...,  0.3501,  0.2815, -0.2102]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0719, -0.1774, -0.1691,  ...,  0.0775,  0.1156,  0.0724],\n",
      "         [ 0.6233,  0.9045,  0.2175,  ...,  0.3566,  0.3470,  0.1101],\n",
      "         [ 2.2676,  0.8069,  0.8379,  ...,  0.9383,  0.7479, -0.0633],\n",
      "         ...,\n",
      "         [-0.0866,  0.0441,  0.1305,  ..., -0.0421,  0.5387, -0.0100],\n",
      "         [-0.1016, -0.2239,  0.1014,  ...,  0.8777, -0.1443, -0.2403],\n",
      "         [-0.0755, -0.2672,  0.0744,  ...,  0.7658, -0.2325, -0.2690]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0120, -0.2905, -0.0609,  ...,  0.2393,  0.0807,  0.2545],\n",
      "         [ 0.6891,  0.5326,  0.5721,  ...,  0.4916,  0.0552,  0.1974],\n",
      "         [ 2.3754,  0.6092,  1.2376,  ...,  0.4350,  0.2173, -0.0254],\n",
      "         ...,\n",
      "         [-0.0685, -0.0888,  0.1295,  ...,  0.0463,  0.0834,  0.0032],\n",
      "         [-0.3336, -0.1299,  0.4940,  ...,  0.8921,  0.0083, -0.3603],\n",
      "         [-0.2670, -0.1555,  0.5069,  ...,  0.7739, -0.0360, -0.3454]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.2491, -0.4744, -0.6638,  ...,  0.6784,  0.0692,  0.6318],\n",
      "         [ 0.9773,  0.2694,  0.6834,  ...,  0.7358, -0.0587, -0.2508],\n",
      "         [ 2.4570,  0.1929,  0.5280,  ...,  0.8177, -0.3536,  0.3959],\n",
      "         ...,\n",
      "         [-0.0299, -0.0556,  0.0128,  ...,  0.0122,  0.0492, -0.0305],\n",
      "         [-0.2571, -0.6463,  0.5759,  ...,  0.7866, -0.2129, -0.4806],\n",
      "         [-0.3378, -0.7823,  0.7324,  ...,  0.7133, -0.2728, -0.3217]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 1.3842e-01, -4.7712e-01, -6.7805e-01,  ...,  2.1091e-02,\n",
      "           3.6605e-01,  6.8329e-01],\n",
      "         [ 8.6152e-01, -3.9222e-01,  5.0176e-01,  ...,  1.4549e-01,\n",
      "          -1.0030e-01, -2.1084e-02],\n",
      "         [ 1.9252e+00,  1.7755e-01,  6.0860e-01,  ...,  6.3161e-01,\n",
      "          -7.8592e-01,  3.3788e-01],\n",
      "         ...,\n",
      "         [-1.9700e-02, -4.1582e-02,  1.8300e-02,  ...,  1.8488e-02,\n",
      "          -1.1955e-03, -4.0495e-02],\n",
      "         [ 2.1375e-01, -2.3824e-01,  5.5817e-01,  ...,  1.2912e-01,\n",
      "           3.1956e-02, -5.5473e-01],\n",
      "         [ 3.4187e-02, -4.2482e-01,  7.9046e-01,  ...,  1.4744e-01,\n",
      "          -1.1718e-02, -4.6000e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[[-1.1236e-03, -6.2980e-01, -7.5593e-01,  ..., -3.5406e-01,\n",
      "           5.1620e-01,  8.3432e-01],\n",
      "         [ 8.7209e-01,  1.7923e-01,  3.8409e-01,  ..., -7.2858e-02,\n",
      "           5.7381e-01, -3.1302e-01],\n",
      "         [ 1.7892e+00,  2.4243e-01,  6.8648e-01,  ...,  5.8090e-01,\n",
      "          -5.0529e-01,  2.4990e-01],\n",
      "         ...,\n",
      "         [ 7.3710e-03, -3.6888e-02, -1.4865e-02,  ...,  2.1112e-03,\n",
      "          -2.5863e-02, -4.3254e-02],\n",
      "         [ 3.9653e-01,  8.7386e-02,  6.4242e-01,  ...,  3.1157e-01,\n",
      "           1.2597e-01, -7.3135e-01],\n",
      "         [ 7.4085e-02, -1.7003e-01,  8.9895e-01,  ...,  1.4422e-01,\n",
      "          -1.0725e-01, -4.7753e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[[-0.1944, -0.4555, -0.5554,  ..., -0.2434,  0.5602,  1.3254],\n",
      "         [ 0.6458,  0.0257,  0.0656,  ...,  0.1465,  0.7280, -0.1494],\n",
      "         [ 1.8307,  0.5324,  0.5346,  ...,  0.6699, -0.1502,  0.0314],\n",
      "         ...,\n",
      "         [-0.0204, -0.0526, -0.0131,  ..., -0.0106,  0.0221, -0.0548],\n",
      "         [ 0.2383,  0.2017,  0.7438,  ...,  0.4617,  0.4650, -0.5875],\n",
      "         [-0.0431, -0.1634,  0.9361,  ...,  0.3602,  0.2430, -0.2707]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0775, -0.2556, -0.5927,  ..., -0.7569,  0.5352,  0.9499],\n",
      "         [ 0.8249,  0.1912, -0.5193,  ..., -0.1167,  0.5201,  0.2266],\n",
      "         [ 1.9575,  0.3809,  0.5592,  ..., -0.2602, -0.2283,  0.1725],\n",
      "         ...,\n",
      "         [ 0.0063, -0.0443,  0.0100,  ..., -0.0475, -0.0209, -0.0860],\n",
      "         [ 0.2387,  0.3124,  0.4745,  ...,  0.3213,  0.4550, -0.7324],\n",
      "         [-0.1408, -0.0661,  0.7161,  ...,  0.1362,  0.1275, -0.4084]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 7.3669e-02,  2.7802e-01, -4.6531e-01,  ..., -6.8014e-01,\n",
      "           1.2423e-01,  6.8809e-01],\n",
      "         [ 8.3667e-01,  5.7842e-01, -4.3332e-01,  ..., -2.9113e-01,\n",
      "           1.5483e-01,  7.2979e-04],\n",
      "         [ 1.4993e+00,  6.6219e-01,  4.6924e-01,  ..., -1.5683e-01,\n",
      "           1.3537e-01, -4.9741e-02],\n",
      "         ...,\n",
      "         [ 1.9897e-03, -3.2836e-03,  1.8384e-02,  ..., -6.8616e-02,\n",
      "          -5.1341e-02, -5.5890e-02],\n",
      "         [ 2.5433e-01,  8.1925e-01,  2.3264e-01,  ...,  2.0128e-01,\n",
      "           2.9325e-01, -7.4243e-01],\n",
      "         [-1.9585e-01,  3.6784e-01,  2.2025e-01,  ...,  1.1274e-01,\n",
      "          -2.1045e-02, -4.6834e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0911,  0.0629, -0.2589,  ..., -0.5322, -0.1506,  0.4456],\n",
      "         [ 0.7633,  0.6224,  0.1768,  ..., -0.0470,  0.1244, -0.4777],\n",
      "         [ 1.2891,  0.4566,  0.7688,  ..., -0.4785,  0.4238, -0.2007],\n",
      "         ...,\n",
      "         [ 0.0013, -0.0241,  0.0210,  ...,  0.0579, -0.0097, -0.0252],\n",
      "         [ 0.0793,  1.0419,  0.5120,  ..., -0.0257,  0.5777, -0.5340],\n",
      "         [-0.3208,  0.4644,  0.4940,  ..., -0.0709,  0.2054, -0.2639]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0889,  0.0888,  0.1771,  ..., -0.6841,  0.1262,  0.2029],\n",
      "         [ 0.4151,  0.6465,  0.1038,  ..., -0.2516,  0.3611, -0.2253],\n",
      "         [ 1.1160,  0.4923,  0.8259,  ..., -0.5608,  0.6617,  0.2079],\n",
      "         ...,\n",
      "         [ 0.0222,  0.0117, -0.0222,  ...,  0.0102, -0.0238,  0.0246],\n",
      "         [ 0.2006,  0.8143,  0.7770,  ..., -0.3115,  0.4668, -0.2893],\n",
      "         [-0.0240,  0.3401,  0.9332,  ..., -0.3934,  0.1192, -0.3032]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0719,  0.2163,  0.0047,  ..., -0.5865,  0.2262,  0.1981],\n",
      "         [ 0.2236,  0.6536, -0.2294,  ..., -0.3547,  0.5517, -0.2367],\n",
      "         [ 1.0410,  0.7755,  1.0335,  ..., -0.5621,  0.5218, -0.0852],\n",
      "         ...,\n",
      "         [ 0.6156,  0.1036, -0.1875,  ..., -0.3799, -0.7008, -0.3500],\n",
      "         [ 0.0791,  0.4287,  0.4147,  ..., -0.2417,  0.2403,  0.0378],\n",
      "         [-0.0165,  0.2459,  0.4566,  ..., -0.2179,  0.1876,  0.0228]]],\n",
      "       grad_fn=<NativeLayerNormBackward>))\n"
     ]
    }
   ],
   "source": [
    "print(\"last_hidden_state: \\n {}\".format(last_hidden_state))\n",
    "print(\"pooler_output: \\n {}\".format(pooler_output))\n",
    "print(\"last_hidden_state: \\n {}\".format(hidden_states))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state shape: \n",
      " torch.Size([1, 7, 768])\n",
      "pooler_output shape: \n",
      " torch.Size([1, 768])\n",
      "last_hidden_state shape: \n",
      " 13\n"
     ]
    }
   ],
   "source": [
    "print(\"last_hidden_state shape: \\n {}\".format(last_hidden_state.size()))\n",
    "print(\"pooler_output shape: \\n {}\".format(pooler_output.size()))\n",
    "print(\"last_hidden_state shape: \\n {}\".format(len(hidden_states)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state elementi: \n",
      " 5376\n",
      "pooler_output elementi: \n",
      " 768\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "numel(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-915d1b6322a9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"last_hidden_state elementi: \\n {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlast_hidden_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pooler_output elementi: \\n {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpooler_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"last_hidden_state elementi: \\n {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: numel(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "print(\"last_hidden_state elementi: \\n {}\".format(torch.numel(last_hidden_state)))\n",
    "print(\"pooler_output elementi: \\n {}\".format(torch.numel(pooler_output)))\n",
    "#print(\"last_hidden_state elementi: \\n {}\".format(torch.numel(hidden_states)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 7, 768])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}