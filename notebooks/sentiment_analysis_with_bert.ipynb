{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Ora faremo un text classification con il bert.\n",
    "Importeremo il dataset imdb e poi chiameremo l'oggetto dataset di nlp di hugginface che dividera per noi\n",
    "i dati in training set e test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlp==0.4.0 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (0.4.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from nlp==0.4.0) (4.49.0)\r\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from nlp==0.4.0) (3.0.0)\r\n",
      "Requirement already satisfied: filelock in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from nlp==0.4.0) (3.0.12)\r\n",
      "Requirement already satisfied: xxhash in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from nlp==0.4.0) (2.0.0)\r\n",
      "Requirement already satisfied: dill in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from nlp==0.4.0) (0.3.3)\r\n",
      "Requirement already satisfied: pandas in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from nlp==0.4.0) (1.2.2)\r\n",
      "Requirement already satisfied: numpy in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from nlp==0.4.0) (1.20.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from nlp==0.4.0) (2.25.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from requests>=2.19.0->nlp==0.4.0) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from requests>=2.19.0->nlp==0.4.0) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from requests>=2.19.0->nlp==0.4.0) (1.26.3)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from requests>=2.19.0->nlp==0.4.0) (3.0.4)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from pandas->nlp==0.4.0) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from pandas->nlp==0.4.0) (2.8.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->nlp==0.4.0) (1.15.0)\r\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement transformer==3.5.1\u001B[0m\r\n",
      "\u001B[31mERROR: No matching distribution found for transformer==3.5.1\u001B[0m\r\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\r\n",
      "Requirement already satisfied: torch==1.7.1+cu110 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (1.7.1+cu110)\r\n",
      "Requirement already satisfied: torchvision==0.8.2+cu110 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (0.8.2+cu110)\r\n",
      "Requirement already satisfied: torchaudio===0.7.2 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (0.7.2)\r\n",
      "Requirement already satisfied: typing-extensions in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from torch==1.7.1+cu110) (3.7.4.3)\r\n",
      "Requirement already satisfied: numpy in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from torch==1.7.1+cu110) (1.20.1)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages (from torchvision==0.8.2+cu110) (8.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nlp==0.4.0\n",
    "!pip install transformer==3.5.1\n",
    "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from nlp import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#!wget https://drive.google.com/uc?id=11_M4ootuT7I1G0RlihcC0cA3Elqotlc- -O mario.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='../data/raw/imdb.csv', split='train')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "nlp.arrow_dataset.Dataset"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'train': Dataset(features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}, num_rows: 70),\n 'test': Dataset(features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}, num_rows: 30)}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora importiamo bert, per il task di classificazione dobbiamo scaricare una versione aposita del bert che si chiama\n",
    "bertForSequenceClassification e deve essere pre allenato e scegliamo la versione base di bert con bert-base-uncased.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Va in portato anche un tokenizer, nella pratica non si usa bertTokenizer ma BertTokenizerFast che in colpo solo\n",
    "tokeniza e da la maschera di attenzione e i tokens id."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 1045, 2293, 3000, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('I love Paris')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Con questo tokenizer possiamo anche tokenizare insiemi di frasi, e possiamo anche chiederli di usare\n",
    "padding inserendo il token [PAD] settando il flag a True chiamtto padding e impostando un valore sulla sequenza\n",
    "massima.\n",
    "Dal momento che sono singole frasi e non hanno frasi conseguenti il 'token_type_ids per ciascune di esse è\n",
    "sempre 0 per ogni token che compone la frase."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [[101, 1045, 2293, 3000, 102], [101, 5055, 4875, 102, 0], [101, 4586, 2991, 102, 0]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0], [1, 1, 1, 1, 0]]}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['I love Paris', 'birds fly', 'snow fall' ], padding= True, max_length= 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def preProcessing(data):\n",
    "    return tokenizer(data['text'], padding=True, truncation= True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ora applichiamo preprocessing ad ogni elemento del training e test set la nostra funzione di preprocessing\n",
    "attraverso il mapping.\n",
    "\n",
    "train_set = train_set.map(preprocess, batched=True,\n",
    "                          batch_size=len(train_set))\n",
    "test_set = test_set.map(preprocess, batched=True, batch_size=len(test_set))\n",
    "\n",
    "train_set.set_format('torch',\n",
    "                      columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_set.set_format('torch',\n",
    "                     columns=['input_ids', 'attention_mask', 'label'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "train_set = train_set.map(preProcessing, batched=True, batch_size=len(train_set))\n",
    "test_set = train_set.map(preProcessing, batched=True, batch_size=len(train_set))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "selezioniamo i dati che ci servono con set_format prendendp le colonne necessarie ovvero:\n",
    "-input_ids\n",
    "-attention_mask\n",
    "-label\n",
    "infine passiamo come argomento anche 'torch' per richiedere i dati in tensori ma per pytorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train_set.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_set.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset(features: {'label': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}, num_rows: 70)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formazione del modello\n",
    "Adesso dobbiamo allenare il nostro bert al compito di classificazione per sentiment analysis che ricordiamo\n",
    "è considerato un task downstream.\n",
    "Per prima cosa dobbiamo importare gli oggetti Trainer, TrainingArguments dalla libreria transformers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Impostiamo la dimensione di batch e il numero di epoche, il warm_up e la regolarizzazione L2 weight_decay"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 2\n",
    "warm_up_steps = 500\n",
    "weight_decay = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size = batch_size,\n",
    "                                  per_device_eval_batch_size= batch_size,\n",
    "                                  num_train_epochs = epochs,\n",
    "                                  warmup_steps= warm_up_steps,\n",
    "                                  weight_decay=weight_decay,\n",
    "                                  output_dir=\"../output\",\n",
    "                                  logging_dir=\"../logs\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "trainer = Trainer(model = model,\n",
    "                  args= training_args,\n",
    "                  train_dataset= train_set,\n",
    "                  eval_dataset = test_set)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/PycharmProjects/Longformer/venv/lib/python3.8/site-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 7.80 GiB total capacity; 6.41 GiB already allocated; 25.19 MiB free; 6.45 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-3435b262f1ae>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/Longformer/venv/lib/python3.8/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, model_path, trial)\u001B[0m\n\u001B[1;32m    773\u001B[0m                         \u001B[0mtr_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    774\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 775\u001B[0;31m                     \u001B[0mtr_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    776\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_total_flos\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloating_point_ops\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    777\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Longformer/venv/lib/python3.8/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtraining_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   1124\u001B[0m                 \u001B[0mscaled_loss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1125\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1126\u001B[0;31m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1127\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1128\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Longformer/venv/lib/python3.8/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[0;32m--> 221\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Longformer/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m    128\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 130\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 7.80 GiB total capacity; 6.41 GiB already allocated; 25.19 MiB free; 6.45 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}